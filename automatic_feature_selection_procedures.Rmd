---
title: "A Review of Automatic Feature Selection Processes in R"
author: "Michael Najarro"
date: "12/5/2020"
output: github_document
---

#*Objective*
This report demonstrates automatic feature selection procedure (AFSP) tools in R. AFSPs expedite the data processing procedures commonly performed prior to implementing Machine Learning or Artifical Intelligence algorithms.

Due to limitations of my hardware and for the purposes of testing AFSPs, I  work with several data sets to demonstrate the following AFSP packages in R:
  1. Boruta
  
  2. LASSO
  
  3. Vtreat
  
  4. Vsurf
  
  5. Genetic algorithm (GA)


# *Boruta*

Written by Miron B. Kursa and Witold R. Rudnicki (https://www.jstatsoft.org/article/view/v036i11).

Boruta aids in selecting the most minimally optimal variables from a large pool of features that maximize classification by implementing wrapper algorthims that avoids the influential effects of selection by accuracy.

Boruta eseentially implements a two sample Z-test, comparing Z scores of the average loss of accuracy generated from a random forest applied to the ranked features of the data set to the Z scores of "shadow features" created from each variable. The shadow features are randomly selected values from a given variable. The comparison essentially evlautes the standing of a given variable relative to its imposter of random noise; if 
its los of accuracy is not smaller than random noise, it is excluded.

I demosntrate an example of Boruta below on the manually selected data of the Kaggle Lending Club analysis report (LC_Analysis.Rmd).

If one has access to more extensive hardware, one could modify and apply these algorithms to the original Kaggle Lending CLub data set, with some minor processing (cleaning steps would be to convert the response variable to a factor format, remove NAS, and depending on the procedure,remove factor variables).

Please note that in phase five of the Boruta AFSP, I implement the same machine learning processes found in the Kaggle data analysis project, to compare the reduced feature analysis to my original analysis. One may skip the immplementation of Boruta (the process took approximately 5 hours using 2nd generation Intel i5 M520 2 core processor with 8 GB of RAM) and load data based on Boruta's recommendation by going directly to phase 5 and implementing its code.


```{r}
library(pacman)
p_load(Boruta,
       tictoc)
```

## **Phase 1: Prepare your training and test data sets**

#### 1.a) load your data and create test and training sets

```{r}
#LCTF <- readRDS(file="./cleaneddata.rds")
refineddf <- readRDS(file = "./data/data_for_rf.rds" )

# create the test data
n <- nrow(refineddf)

# create the test data
test <- sample.int(n, size = round(0.25 * n))
test_set <- refineddf[test, ]
nrow(test_set)

#create the training data
train_set <- refineddf[-test,]
nrow(train_set)
```


## **Phase 2: Implement Boruta**

### 2.a) Implement Boruta on the training set

Implementing Boruta on a small laptop with 8 GB of ram and only two core processor took approximately 5.33 hours. Please consider your hardware and timing before running this step. 

```{r}
tic()
boruta.train <- Boruta(chance_default~., data = train_set, doTrace =  2)
toc()
```


### 2.b) assess Boruta's results

```{r}
# what were boruta's results?
boruta.train

# the decisions made per variable
boruta.train$finalDecision
```


a plot of the variable importance, based on the importance history output.

```{r}
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory), function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i] )
names(lz) <- colnames(boruta.train$ImpHistory)
labels <- sort(sapply(lz,median))
axis(side = 1,
     las=2,
     labels = names(labels),
     at = 1:ncol(boruta.train$ImpHistory),
     cex.axis = 0.5)
```

```{r}
# pull out the importance history as a separate data frame
#a <- as.data.frame(boruta.train$ImpHistory)

# within each column, there may be infinity and negative infinity values. So you have to go through each column and pull out the numeric values.
#lz<-lapply(1:ncol(a), function(x) a[is.finite(boruta.train$ImpHistory[,x]),x] )

# convert the list to a data frame and label columns
#lz<- data.frame(matrix(unlist(lz), nrow = 99, byrow = FALSE))
#names(lz) <- colnames(a[,c(1:43)])

# now plot the summary of each element of lz
#ggplot2(data = lz, mapping = aes(x = ))

```


## **Phase 3: Use Boruta's recommendation to reduce the number of predictor variables forrandom forest model training**

```{r}
p_load(tidyverse, magrittr)

str(boruta.train$finalDecision)
levels(boruta.train$finalDecision)

# create a new vector of the final decisions
b <- c(boruta.train$finalDecision)

# identify the names of the elements that were confirmed.
j<-names(which(b==2))

#keep these columns in the original data set by tossing out others.
refineddf <- refineddf %>%
  select(all_of(j), chance_default)

saveRDS(object = refineddf, file = "./boruta_train_data.RDS")
```


### phase 4: evalution

Boruta reduced the number of variables from 45 to 37.

```{r}
bdata<- readRDS(file = "./data/boruta_train_data.RDS")
```


### phase 5: Run bortua features through random forest and compare to Kaggle data analysis project.
I here now Implement a Random Forest algorithm for classification on the Boruta reduced-feeatured data.








# LASSO

The Least absolute shrinkage and selection operator, written by Robert Tibshirani (https://beehive.cs.princeton.edu/course/read/tibshirani-jrssb-1996.pdf)

LASSO's goal is 

## **Phase 1: Load your data into the environment**

```{r}
refineddf <- readRDS(file = "./data/data_for_rf.rds" )
```


## **Phase 2: Process data to contain strictly numeric data**

Note that LASSO found in the glmnet package requires the use of all numeric values.

```{r}
a<- refineddf %>%
  select_if(is.numeric)

# 1 = safe, 0 = risk
a$chance_default <- as.numeric(as.factor(refineddf$chance_default)) - 1
x <- model.matrix(chance_default~., data = a)[,-33]
y <- a$chance_default
```


## **Phase 3: Implement LASSO**

```{r}
library(glmnet)

lasso_mod <- glmnet(x, y, alpha = 0)

set.seed(1443)
lasso_cvfit <- cv.glmnet(x, y, alpha=0)
lasso_cvfit$lambda.min # selected labda value

#plot with alpha threshold
plot(lasso_mod, xvar = "lambda")
abline(v=log(lasso_cvfit$lambda.min))

 
coef(lasso_cvfit, s="lambda.min")
lasso_coefs <- as.numeric(coef(lasso_cvfit, s="lambda.min"))
#sum(abs(lasso_coefs) > 0)
```



# Vtreat

Written by John Mount and Nina Zumel (https://github.com/WinVector/vtreat, https://arxiv.org/pdf/1611.09477.pdf).

A package conaining methods for preprocessing data for supervised machine learning or predictive modeling.

Vtreat takes a data frame with defined refined response and predictor variables and cleans the data by transforming all non-numeric columns to type numeric. Vtreat can deal with categorical variables that contain a large number of levels or exposure to new levels in the test data through sub modeling procedures.

Vtreat has a variety of built in methods to deal with processing issues, and several procedures depending on the type of machine learning algorithm and purpose one wishes to perform.

There are two general approaches to using Vtreat:

  1. Using the three prong approach commonly found in Machine learning, where data is split into to training, validation, and testing (in Vtreat examples it is calibration, training, testing).
  
  2. the more statistically efficient manner is to merge the training and validation steps through cross validation approaches, essentially limiting the split in data to two phases.

I first focus on the first, and recommended approach, followed by the second, and more complicated approach.

## Cross Validation approach

### **Phase 1: load initial data**

#### 1.a) load your libraries and data
As a proof of demonstration, I will use the LCTF10 data used in my Kaggle Lending Club Analysis project.

```{r}
# libraries
library(pacman)
p_load(vtreat, car, magrittr, tidyverse, rqdatatable)

LCTF10 <- readRDS(file = "./data/cleaneddata.rds" )
```

#### 1.b) Pre cleaning of data before Vtreat

Due to limitations in hardware and to prepare the data for vtreat, I first need to modify the response variable.

```{r}
# convert the response to a factor
LCTF10$borrower_status <- as.factor(LCTF10$borrower_status)

# create a new vector that avoids the space in the response variable's levels
possible_default <- as.factor(recode(as.vector(LCTF10$borrower_status), "risky client"='risk', "safe client"='safe'))

# add the vector to LCTF10
LCTF10 <- LCTF10 %>%
  mutate(chance_default = possible_default) %>%
  select(-borrower_status)

#convert the vector to a binary outcome, with risk = 1, safe = 0.
LCTF10$chance_default <- as.numeric(ifelse(LCTF10$chance_default == "risk", 1, 0))
```


I then remove a few initial columns that are filled with or over 50% NAs.

```{r}
# create a function to determine which variables have
# less than or equal to 50% legit data:
d <- rep(0, ncol(LCTF10))
bad <- as.integer(rep(0, ncol(LCTF10)))
result<- as.character(rep(0, nrow(LCTF10)))

assess_bad_data<- function(df) {
d <<- (colSums(!is.na(LCTF10))/nrow(LCTF10))
bad <<- (which(d <=.50))
result <<- (colnames(LCTF10[bad]))
}
assess_bad_data(LCTF10)

#how many bad columns are there?
length(result)

#which are the bad columns(by column number)?
head(bad)

# toss the bad variables from td.
LCTF10 <- LCTF10[-(bad)]

rm(bad,d,possible_default,result,assess_bad_data)
```


### **Phase 2: Build your transform**

Within the data are several columns containing NAs, and categorical variables with extreme cardinality (categorical levels with many levels.)

```{r}
library(Amelia)
missmap(LCTF10)
```

#### 2.a) generate sub groups

Vtreat documenation recommends splitting the data into two subsets to prevent overfitting: training and test sets. I perform these splits below.

```{r}
set.seed(16819)
# perform a 10-fold cross validation on the original data.
# note: each fold is split into 10 sets.

dat <- kWayStratifiedY(nrow(LCTF10), 10, LCTF10, LCTF10$chance_default)

# use the first fold to obtian the rows marked as test and training data.
fold1 <- dat[[1]]
train <- fold1$train # training set indices
test <- fold1$app 

#now pull the rows out
t_train <- LCTF10[train, ,drop=FALSE] 
t_test <- LCTF10[test, ,drop=FALSE]

outcome <- 1
vars <- setdiff(colnames(t_train), outcome)
```


#### 2.b) now create the treatment plan from training data

```{r}
# initiate parrallelization (parallel clusters)
q <-parallel::detectCores()
pc <- parallel::makeCluster(q)

# generate the treatment plan  from training data
ytarget <- 1
crossframe_exp <- mkCrossFrameCExperiment(t_train,
                                           varlist = vars,
                                           outcomename = 'chance_default',
                                           outcometarget = ytarget,
                                           parallelCluster = pc)
```


##### 2.c) gather the treatment plan and the treated training data

```{r}
# pull out the treated plan
treatment_plan <- crossframe_exp$treatments

# pull out the training data that receieved the treatment plan
train_treated <- crossframe_exp$crossFrame

# memory bogging down; tidy up
rm(LCTF10,dat,fold1,pc,q,outcome,test,train,ytarget)
```


#### 2.d) investigate the recommended features & the treated data

check out the treatment plan by looking at the score frame.

Each row of the score frame represents a transformed feature of the original data,indicated by the name in the `varName` column. 

Note that the score frame contains all the variables listed in the original data set, plus additional variables containing an original column concatenated with prefixes of `isBAD`, `catB`, and `catP`.

Transformed features without any prefix are the original features but without any missing values (NA).

`isBAD` features are new, meta data features that contain binary representations of the presence or absence of data of their ancestral feature per record.

`catB` feature is a new meta feature that contains the logit score of each response level of a factor variable and is considered the "impact" of the given level of a particular record.

`catP` is a new meta feature that displays the percent of representation, that is the "prevalence" of a given level in a factor variable.

The `rsq` and `sig` column represents the r-squared measure; the percent of variance explained by the model that the treatment plan devised and the significan of that measure.

Also notice that the high cardinality variables are divided up such that each level obtains its own binary feature. This format is analogous to dummy variables in traditional regression modeling.

```{r}
sf <- treatment_plan$scoreFrame
head(sf)

#how many of the features are there?
nrow(sf)
table(sf$code)

# how many columns should be used and others tossed?
#about 149 out of 282 should be kept.
table(sf$recommended)

#pull all your selected variables from the scor frame
sellvars <- sf$varName

#Now pull out all of the recommended variables
#model_vars <- sf$varName[sf$recommended]
```


The changes seen in the score fame are applied to the treated trained data. Investigate the cleanliness of the data; notice there are no NAs and no factor variables anymore.

```{r}
# count the number of missing values in each column
nNAs <- sapply(train_treated,
              function(x) sum(is.na(x)))
summary(nNAs)

# count the number of levels in each categorical column 
v <- train_treated %>%
  select_if(is.factor) %>%
  sapply(function(x) length(levels(x)))

summary(v)
```


#### 2.e) Now reduce your data frame in accordance with the recommendations of the treatment plan

```{r}
# pull out the recommended rows
model_vars <- sf$varName[sf$recommended]

# now filter the treated trained data based on recommended columnms.
cleaned_train <- train_treated[model_vars]
cleaned_train$chance_default <- train_treated$chance_default
cleaned_train$chance_default <- as.factor(ifelse(LCTF10$chance_default ==  1, 'risk', 'safe')
```



#### 2.f) Build the random forest model on the treated training data, then run it.
```{r}
#build your random forest on the cleaned data.
p_load(car, randomForest, caret, e1071)

# create random forest model.,
set.seed(33792)
rf_train = randomForest(chance_default~., data=cleaned_train)


# 

```




#### 2.g) Apply your treatment plan to the test data
```{r}
train_treated <-vtreat::prepare(treatment_plan,
                  train,
                   parallelCluster = pc,
                   pruneSig=NULL)
```



#### 2.h) apply your training model to the test data.

```{r}

```






#### 2.a) generate sub groups

Vtreat documenation recommends splitting the data into three subsets to prevent overfitting: calibraition, training, and test sets. I perform these splits below.

```{r}
set.seed(16819)
LCTF10$group <- base::sample(c("cal",
                               "train",
                               "test"),
                             nrow(LCTF10),
                             replace=TRUE,
                             prob=c(0.5,
                                    0.3,
                                    0.2))

cal <- LCTF10[LCTF10$group =='cal', , drop=FALSE]

train <- LCTF10[LCTF10$group =='train', , drop=FALSE]

test <- LCTF10[LCTF10$group =='test', , drop=FALSE]

trainall <- LCTF10[LCTF10$group %in% c('cal', 'train'), , drop=FALSE]

outcome <- 'risk'
vars <- setdiff(colnames(trainall), outcome)
```


#### 2.b) create a pivot chart of the final outcomes

```{r}
table(cal$chance_default)
prop.table(table(cal$chance_default))
```


### 2.c) create the treatment plan on calibrated data

The next step is to create a designed treatment plan from the calibration data set. 

several versions of the design treatment function; use `designTreatmentsC` for classification schemes.

```{r}
pc <- parallel::makeCluster(parallel::detectCores())

treatment_plan <- vtreat::designTreatmentsC(cal,
                 varlist = vars,
                 outcomename = 'chance_default',
                 outcometarget='risk',
                 verbose=FALSE,
                 parallelCluster = pc)
```


Now check out the treatment plan by looking at the score frame.

Each row of the score frame represents a transformed feature of the original data,indicated by the name in the `varName` column. 

Note that the score frame contains all the variables listed in the original data set, plus additional variables containing an original column concatenated with prefixes of `isBAD`, `catB`, and `catP`.

Transformed features without any prefix are the original features but without any missing values (NA).

`isBAD` features are new, meta data features that contain binary representations of the presence or absence of data of their ancestral feature per record.

`catB` feature is a new meta feature that contains the logit score of each response level of a factor variable and is considered the "impact" of the given level of a particular record.

`catP` is a new meta feature that displays the percent of representation, that is the "prevalence" of a given level in a factor variable.

The `rsq` and `sig` column represents the r-squared measure; the percent of variance explained by the model that the treatment plan devised and the significan of that measure.

needSplit....

Also notice that the high cardinality variables are divided up such that each level obtains its own binary feature. This format is analogous to dummy variables in traditional regression modeling.

```{r}
sf <- treatment_plan$scoreFrame
head(sf)
```

```{r}
#how many of the features are there?
nrow(sf)
table(sf$code)

# how many columns should be used and others tossed?
#about 149 out of 282 should be kept.
table(sf$recommended)

#pull all your variables
sellvars <- sf$varName

#Now pull out all of the recommended variables
#model_vars <- sf$varName[sf$recommended]
```


### 3.e) apply the treatment plan to the training data to prepare the model

```{r}
train_treated <-vtreat::prepare(treatment_plan,
                  train,
                   parallelCluster = pc,
                   pruneSig=NULL)
```


check out the train treated data.

```{r}
head(train_treated)
```





